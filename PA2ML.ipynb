{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e0d0fbe9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mateu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\mateu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw to\n",
      "[nltk_data]     C:\\Users\\mateu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\mateu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "#imports the necessary libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import svm\n",
    "import gensim\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw')\n",
    "nltk.download('omw-1.4')\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.test.utils import common_texts\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2b2b9498",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "#creates a function to preprocess text\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    text = re.sub(r\"won\\'t\", \"will not\", text)\n",
    "    text = re.sub(r\"can\\'t\", \"can not\", text)\n",
    "    text = re.sub(r\"n\\'t\", \" not\", text)\n",
    "    text = re.sub(r'[^a-z0-9\\s]', '', text)\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    text = ' '.join(tokens)\n",
    "    return text\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8735fdb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "#reads the data from excel sheets and runs the preprocessing function through the data\n",
    "dfTrain = pd.read_csv(\"train.csv\")\n",
    "dfTest = pd.read_csv(\"test.csv\")\n",
    "dfTrain = dfTrain.sample(frac=0.3, random_state=40)\n",
    "dfTrain['Text'] = dfTrain['Text'].apply(preprocess_text)\n",
    "dfTest['Text'] = dfTest['Text'].apply(preprocess_text)\n",
    "\n",
    "y_train = dfTrain['Sentiment']\n",
    "y_test = dfTest['Sentiment']\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ecb344b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "#Bag-of-Words\n",
    "\n",
    "count = CountVectorizer()\n",
    "X_Bagtrain = count.fit_transform(dfTrain['Text'])\n",
    "X_Bagtest = count.transform(dfTest['Text'])\n",
    "y_train = dfTrain['Sentiment']\n",
    "y_test = dfTest['Sentiment']\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "200c61aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "#TF-IDF\n",
    "\n",
    "tfidf = TfidfVectorizer()\n",
    "X_TFtrain = tfidf.fit_transform(dfTrain['Text'])\n",
    "X_TFtest = tfidf.transform(dfTest['Text'])\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5f64f340",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "df = pd.concat([dfTrain, dfTest], ignore_index=True)\n",
    "tokenized_train = [nltk.word_tokenize(text) for text in dfTrain['Text']]\n",
    "model = Word2Vec(sentences=df['Text'], vector_size=100, window=5, min_count=1, workers=4)\n",
    "tokenized_test = [nltk.word_tokenize(text) for text in dfTest['Text']]\n",
    "\n",
    "TEvectorized_data = []\n",
    "#for loop that iterates through the testing data\n",
    "for sentence in dfTest['Text']:\n",
    "    sentence_list = nltk.word_tokenize(sentence)\n",
    "    temp_vector_list = []\n",
    "    for word in sentence_list:\n",
    "        if word in model.wv:\n",
    "            word_vector = model.wv[word]\n",
    "            temp_vector_list.append(word_vector)\n",
    "    if temp_vector_list:\n",
    "        averaged_vector = np.average(temp_vector_list, axis=0)\n",
    "        TEvectorized_data.append(averaged_vector)\n",
    "    else:\n",
    "        TEvectorized_data.append(np.zeros(100))\n",
    "\n",
    "#for loop that goes through the training data\n",
    "TRvectorized_data = []\n",
    "for sentence in dfTrain['Text']:\n",
    "    sentence_list = nltk.word_tokenize(sentence)\n",
    "    temp_vector_list = []\n",
    "    for word in sentence:\n",
    "        if word in model.wv:\n",
    "            word_vector = model.wv[word]\n",
    "            temp_vector_list.append(word_vector)\n",
    "    if temp_vector_list:\n",
    "        averaged_vector = np.average(temp_vector_list, axis=0)\n",
    "        TRvectorized_data.append(averaged_vector)\n",
    "    else:\n",
    "        TRvectorized_data.append(np.zeros(100))\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "521764b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#imports models for classification\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix, roc_curve, auc, classification_report\n",
    "\n",
    "\n",
    "#defines names for the models\n",
    "lcBag = LogisticRegression(max_iter=1000)\n",
    "lcTF = LogisticRegression(max_iter=1000)\n",
    "lcW2V = LogisticRegression(max_iter=1000)\n",
    "neigh = KNeighborsClassifier(n_neighbors=3)\n",
    "svc = SVC(probability=True)\n",
    "rfc = RandomForestClassifier()\n",
    "lcBag.fit(X_Bagtrain, y_train)\n",
    "lcTF.fit(X_TFtrain, y_train)\n",
    "lcW2V.fit(TRvectorized_data, y_train)\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d5fdd10a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.90      0.78       177\n",
      "           1       0.86      0.60      0.71       182\n",
      "\n",
      "    accuracy                           0.75       359\n",
      "   macro avg       0.77      0.75      0.74       359\n",
      "weighted avg       0.77      0.75      0.74       359\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.97      0.73       177\n",
      "           1       0.93      0.34      0.50       182\n",
      "\n",
      "    accuracy                           0.65       359\n",
      "   macro avg       0.76      0.66      0.62       359\n",
      "weighted avg       0.76      0.65      0.61       359\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.69      0.58       177\n",
      "           1       0.52      0.32      0.40       182\n",
      "\n",
      "    accuracy                           0.51       359\n",
      "   macro avg       0.51      0.51      0.49       359\n",
      "weighted avg       0.51      0.51      0.49       359\n",
      "\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#compares the performance of bag of words, tf-idf, and word2vec\n",
    "y_lcB_predicted = lcBag.predict(X_Bagtest)\n",
    "y_lcB_pred_proba = lcBag.predict_proba(X_Bagtest)\n",
    "y_lcT_predicted = lcBag.predict(X_TFtest)\n",
    "y_lcT_pred_proba = lcBag.predict_proba(X_TFtest)\n",
    "y_lcW_predicted = lcW2V.predict(TEvectorized_data)\n",
    "y_lcW_pred_proba = lcW2V.predict_proba(TEvectorized_data)\n",
    "print(classification_report(y_test, y_lcB_predicted))\n",
    "print(classification_report(y_test, y_lcT_predicted))\n",
    "print(classification_report(y_test, y_lcW_predicted))\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7e1c30fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mateu\\anaconda3\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.86      0.68       177\n",
      "           1       0.72      0.35      0.47       182\n",
      "\n",
      "    accuracy                           0.60       359\n",
      "   macro avg       0.64      0.60      0.57       359\n",
      "weighted avg       0.64      0.60      0.57       359\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dfTrain = dfTrain.sample(frac=0.07, random_state=40)\n",
    "y_train = dfTrain['Sentiment']\n",
    "count = CountVectorizer()\n",
    "X_Bagtrain = count.fit_transform(dfTrain['Text'])\n",
    "X_Bagtest = count.transform(dfTest['Text'])\n",
    "y_train = dfTrain['Sentiment']\n",
    "\n",
    "#since bag-of-words was the best model, this code uses the data from bag-of-words to test using other models\n",
    "neigh.fit(X_Bagtrain, y_train)\n",
    "y_neigh_predicted = neigh.predict(X_Bagtest)\n",
    "y_neigh_pred_proba = neigh.predict_proba(X_Bagtest)\n",
    "print(classification_report(y_test, y_neigh_predicted))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4e9621f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.99      0.74       177\n",
      "           1       0.97      0.34      0.50       182\n",
      "\n",
      "    accuracy                           0.66       359\n",
      "   macro avg       0.78      0.66      0.62       359\n",
      "weighted avg       0.78      0.66      0.62       359\n",
      "\n"
     ]
    }
   ],
   "source": [
    "svc.fit(X_Bagtrain, y_train)\n",
    "y_svc_predicted = svc.predict(X_Bagtest)\n",
    "y_svc_pred_proba = svc.predict_proba(X_Bagtest)\n",
    "print(classification_report(y_test, y_svc_predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9d36ce21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.98      0.71       177\n",
      "           1       0.92      0.26      0.41       182\n",
      "\n",
      "    accuracy                           0.62       359\n",
      "   macro avg       0.74      0.62      0.56       359\n",
      "weighted avg       0.75      0.62      0.56       359\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rfc.fit(X_Bagtrain, y_train)\n",
    "y_rfc_predicted = rfc.predict(X_Bagtest)\n",
    "y_rfc_pred_proba = rfc.predict_proba(X_Bagtest)\n",
    "print(classification_report(y_test, y_rfc_predicted))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
